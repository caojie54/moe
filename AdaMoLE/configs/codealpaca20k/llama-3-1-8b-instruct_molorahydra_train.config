--model_path=meta-llama/Llama-3.1-8B-Instruct
--data_path=/data/workspace/projects/moe/datasets/codealpaca20k
--peft_type=molora
--lora_rank=64
--target_modules
q_proj
k_proj
v_proj
o_proj
down_proj
--num_experts=8
--max_length=500
--batch_size=2
--gradient_accumulation_steps=8
--num_train_epochs=1
--learning_rate=1e-4
--lr_scheduler_type=constant_with_warmup
--warmup_steps=200
--weight_decay=0.0
--hydra
