{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.zeros_(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOELoraLayer(nn.Module):\n",
    "    def __init__(self, dim, r, expert_num, hydra=False):\n",
    "        super().__init__()\n",
    "        self.expert_num = expert_num\n",
    "        self.hydra = hydra # hydra lora\n",
    "\n",
    "        self.router = nn.Linear(dim, expert_num, bias=False)\n",
    "\n",
    "        if hydra:\n",
    "            self.lora_A = nn.Linear(dim, r, bias=False)\n",
    "        else:\n",
    "            self.lora_A = nn.ModuleList()\n",
    "            for i in range(expert_num):\n",
    "                self.lora_A.append(nn.Linear(dim, r, bias=False))\n",
    "            \n",
    "        self.lora_B = nn.ModuleList()\n",
    "        for i in range(expert_num):\n",
    "            self.lora_B.append(nn.Linear(r, dim, bias=False))\n",
    "\n",
    "        # initial lora B to zeros\n",
    "        for linear in self.lora_B:\n",
    "            nn.init.zeros_(linear.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        route_weight = nn.functional.softmax(self.router(x), dim=-1, dtype=torch.float32).to(x.dtype)\n",
    "        # try lora_alpha\n",
    "        for i in range(self.expert_num):\n",
    "            if self.hydra:\n",
    "                x = x + torch.unsqueeze(route_weight[:,:,i], -1) * self.lora_B[i](self.lora_A(x))\n",
    "            else:\n",
    "                x = x + torch.unsqueeze(route_weight[:,:,i], -1) * self.lora_B[i](self.lora_A[i](x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "moelora = MOELoraLayer(10,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['router.weight', 'lora_A.0.weight', 'lora_A.1.weight', 'lora_A.2.weight', 'lora_A.3.weight', 'lora_A.4.weight', 'lora_B.0.weight', 'lora_B.1.weight', 'lora_B.2.weight', 'lora_B.3.weight', 'lora_B.4.weight'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moelora.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,4)\n",
    "a = torch.randn(4,2)\n",
    "a1= torch.randn(4,2)\n",
    "a2= torch.randn(4,2)\n",
    "\n",
    "b1 = torch.randn(2,4)\n",
    "b2= torch.randn(2,4)\n",
    "\n",
    "relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0131,  0.8114, -0.1674,  0.1921],\n",
       "        [ 0.3173,  1.5902,  1.2141,  0.6184],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(relu(x @ a1) @ b1)*0.4 + (relu(x @ a2) @ b2)*0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0131,  0.8114, -0.1674,  0.1921],\n",
       "        [ 0.3173,  1.5902,  1.2141,  0.6184],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(x @ a1) @ (b1*0.4) + relu(x @ a2) @ (b2*0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5762, -0.2077, -0.6032, -0.1846],\n",
       "        [-0.3817, -0.1376, -0.3996, -0.1223],\n",
       "        [ 0.0645, -0.0288,  0.0293, -0.0455]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(x @ a) @ ( b1*0.4) + relu(x @ a) @ (b2*0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5762, -0.2077, -0.6032, -0.1846],\n",
       "        [-0.3817, -0.1376, -0.3996, -0.1223],\n",
       "        [ 0.0645, -0.0288,  0.0293, -0.0455]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(x @ a) @ ( b1*0.4 + b2*0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "ta = torch.FloatTensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1876773/2441912604.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nn.functional.softmax(ta)*4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1282, 0.3486, 0.9475, 2.5757])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(ta)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta.unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# math length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLaMA3_lora_bias.llama import Tokenizer\n",
    "model_path = '/home2/caojie/pretrain_models/Meta-Llama-3-8B/'\n",
    "tokenizer = Tokenizer(model_path= f\"{model_path}/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'/home2/caojie/projects/LLM-Adapters/ft-training_set/math_14k.json', 'r') as f:\n",
    "# with open(f'/home2/caojie/projects/LLM-Adapters/ft-training_set/commonsense_170k.json', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Please answer the following question with true or false, question: do good samaritan laws protect those who help at an accident?\\n\\nAnswer format: true/false',\n",
       " 'input': '',\n",
       " 'output': 'the correct answer is true',\n",
       " 'answer': 'true'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average tokens:184.21255656921198\n",
      "average tokens:127.19854895481646\n"
     ]
    }
   ],
   "source": [
    "input_lens=[]\n",
    "output_lens=[]\n",
    "for x in data:\n",
    "    input_lens.append(len(tokenizer.encode(x['instruction']+x['output'], bos=False, eos=False)))\n",
    "    output_lens.append(len(tokenizer.encode(x['output'], bos=False, eos=False)))\n",
    "print(f'average tokens:{sum(input_lens)/len(input_lens)}')\n",
    "print(f'average tokens:{sum(output_lens)/len(output_lens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddSub: accuracy 356  0.9012658227848102\n",
      "AQuA: accuracy 92  0.36220472440944884\n",
      "gsm8k: accuracy 1024  0.7763457164518575\n",
      "MultiArith: accuracy 591  0.985\n",
      "SingleEq: accuracy 491  0.9665354330708661\n",
      "SVAMP: accuracy 810  0.81\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import copy\n",
    "import os\n",
    "def extract_answer_number(dataset, sentence: str) -> float:\n",
    "    dataset = dataset.lower()\n",
    "    if dataset in [\"multiarith\", \"addsub\", \"singleeq\", \"gsm8k\", \"svamp\"]:\n",
    "        sentence = sentence.replace(',', '')\n",
    "        pred = [s for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n",
    "        if not pred:\n",
    "            return float('inf')\n",
    "        pred_answer = float(pred[-1])\n",
    "    else:\n",
    "        raise NotImplementedError(' not support dataset: {}'.format(dataset))\n",
    "    if isinstance(pred_answer, str):\n",
    "        try:\n",
    "            pred_answer = float(pred_answer)\n",
    "        except ValueError as e:\n",
    "            pred_answer = float('inf')\n",
    "    return pred_answer\n",
    "\n",
    "\n",
    "def extract_answer_letter(sentence: str) -> str:\n",
    "    sentence_ = sentence.strip()\n",
    "    pred_answers = re.findall(r'A|B|C|D|E', sentence_)\n",
    "    if pred_answers:\n",
    "        if not pred_answers:\n",
    "            return ''\n",
    "        return pred_answers[0]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "test_dataset_l=\"AddSub AQuA gsm8k MultiArith SingleEq SVAMP\"\n",
    "for dataset in test_dataset_l.split():\n",
    "    save_path=f\"/home2/caojie/outputs/LLaMA3-1_lora_bias/math_14k/b32_epoch3_warme1_lorar8_loraQ,K,V,O,FFN_UP_blr6e-3_maxseq300_flashatt2False_bf16True_/{dataset}_predict_mingen120.jsonl\"\n",
    "    with open(save_path, 'r') as f:\n",
    "        data_l = f.readlines()\n",
    "    data_l = [json.loads(one) for one in data_l]\n",
    "    total = len(data_l)\n",
    "    correct = 0\n",
    "    miss = 0.001\n",
    "    for data in data_l:\n",
    "        label = data.get('answer')\n",
    "        flag = False\n",
    "        if dataset.lower() in ['aqua']:\n",
    "            predict = extract_answer_letter(data.get('generate'))\n",
    "            if label == predict:\n",
    "                correct += 1\n",
    "                flag = True\n",
    "        else:\n",
    "            if isinstance(label, str):\n",
    "                label = float(label)\n",
    "            predict = extract_answer_number(dataset, data.get('generate'))\n",
    "            if abs(label - predict) <= miss:\n",
    "                correct += 1\n",
    "                flag = True\n",
    "        new_data = copy.deepcopy(data)\n",
    "        new_data['pred'] = predict\n",
    "        new_data['flag'] = flag\n",
    "\n",
    "        directory = os.path.dirname(save_path)\n",
    "        with open(os.path.join(directory, f'{dataset}_predict_check.jsonl'), 'a', encoding='utf-8') as f:\n",
    "            json_data = json.dumps(new_data, ensure_ascii=False)\n",
    "            f.write(json_data+'\\n')\n",
    " \n",
    "    print(f'{dataset}: accuracy {correct}  {correct / total}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
